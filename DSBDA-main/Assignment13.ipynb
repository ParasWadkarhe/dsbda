
1. Variable Declaration in Scala:
var Var1: String = "Raj"
val Var2: String = "Raj"

var Var4 = 2
var Var5 = 3
Var4 + Var5 // Output: 5

Var4 == Var5 // Output: false
Output:
Var4: Int = 2
Var5: Int = 3
res1: Int = 5
res2: Boolean = false

2. Using If-Else Expression in Scala:
var Var3 = 1
if (Var3 == 1) {
  println("True")
} else {
  println("False")
}
Output:
True

3. Iteration in Scala using For Loop:
for (a <- 1 to 10) {
  println("Value of a: " + a)
}
Output:
Value of a: 1
Value of a: 2
Value of a: 3
Value of a: 4
Value of a: 5
Value of a: 6
Value of a: 7
Value of a: 8
Value of a: 9
Value of a: 10

4. Declaring a Simple Function in Scala:
def mul2(m: Int): Int = m * 10

mul2(2)  // Output: 20
Output:
res9: Int = 20

5. Working with Arrays in Scala:
var name = Array("Faizan", "Swati", "Kavya", "Deepak", "Deepak")
name(0) = "jal"
name(1) = "Faizy"
name(2) = "Expert in deep learning"
println(name.mkString(", "))
Output:
jal, Faizy, Expert in deep learning, Deepak, Deepak

6. Simple "Hello World" Program in Scala:
object HelloWorld {
  def main(args: Array[String]): Unit = {
    println("Hello, world!")
  }
}
Output:
Hello, world!

7. Scala Program using Apache Spark:
// Importing Spark libraries
import org.apache.spark.sql.SparkSession

object SparkExample {
  def main(args: Array[String]): Unit = {
    // Initialize Spark session
    val spark = SparkSession.builder()
      .appName("Simple Spark Example")
      .master("local")
      .getOrCreate()

    // Create a simple DataFrame
    val data = Seq(("Raj", 24), ("Teja", 40), ("Raman", 30), ("Priya", 25))
    val df = spark.createDataFrame(data).toDF("Name", "Age")

    // Show the DataFrame
    df.show()

    // Perform a simple transformation: filtering rows where age is greater than 28
    val filteredDf = df.filter(df("Age") > 28)

    // Show the transformed DataFrame
    filteredDf.show()

    // Stop Spark session
    spark.stop()
  }
}
Expected Output:
+-----+---+
| Name|Age|
+-----+---+
| Raj | 24|
| Teja | 40|
| Raman | 30|
| Priya | 25|
+-----+---+

+-----+---+
| Name|Age|
+-----+---+
| Teja | 40|
| Raman | 30|
+-----+---+

8. Install and Check Java Version:
$ java -version
Output:
java version "1.8.0_251"
Java(TM) SE Runtime Environment (build 1.8.0_251-b08)
Java HotSpot(TM) 64-Bit Server VM (build 25.251-b08, mixed mode)

9. Install Scala on Ubuntu:
$ cd ~/Downloads
$ wget http://www.scala-lang.org/files/archive/scala-2.11.7.deb
$ sudo dpkg -i scala-2.11.7.deb
$ scala â€“version
Output:
Scala code runner version 2.11.7 -- Copyright 2002-2015, LAMP/EPFL


Scala Program using Apache Spark:
// Importing Spark libraries
import org.apache.spark.sql.SparkSession

object SparkExample {
  def main(args: Array[String]): Unit = {
    // Initialize Spark session
    val spark = SparkSession.builder()
      .appName("Simple Spark Example")
      .master("local")
      .getOrCreate()

    // Create a simple DataFrame
    val data = Seq(("Raj", 24), ("Teja", 40), ("Raman", 30), ("Priya", 25))
    val df = spark.createDataFrame(data).toDF("Name", "Age")

    // Show the DataFrame
    df.show()

    // Perform a simple transformation: filtering rows where age is greater than 28
    val filteredDf = df.filter(df("Age") > 28)

    // Show the transformed DataFrame
    filteredDf.show()

    // Stop Spark session
    spark.stop()
  }
}

Expected Output:
+-----+---+
| Name |Age|
+-----+---+
| Raj | 24|
| Teja | 40|
| Raman | 30|
| Priya | 25|
+-----+---+

+-----+---+
| Name |Age|
+-----+---+
| Teja | 40|
| Raman | 30|
+-----+---+



